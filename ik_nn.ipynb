{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.autograd import Variable"
      ],
      "outputs": [],
      "metadata": {
        "id": "TTvPtGgBepel"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling the Manipulator\n",
        "\n",
        "To start, the classes \"Manipulator\" and \"Link\" were created that allows the representation of the specifications of manipulators. These classes will be used later on to generate the data required for the manipulators we will train our models to compute the inverse kinematics for. For an instance of this class to be instantiated, the Denavit-Hartenberg (DH) parameters of the manipulator's links will have to be provided. This allows for the forward kinemtics to be computed in order to find the position of the end effector, given the link's angles."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "class Link():\n",
        "    def __init__(self,theta_offset,d,alpha,a):\n",
        "        self.theta_offset = theta_offset\n",
        "        self.d = d\n",
        "        self.alpha = alpha\n",
        "        self.a = a\n",
        "    def get_transformation(self, theta):\n",
        "        theta = theta+self.theta_offset\n",
        "        transformation_matrix = np.array([\n",
        "            [cos(theta), -sin(theta) * cos(self.alpha), sin(theta) * sin(self.alpha), self.a * cos(theta)],\n",
        "            [sin(theta), cos(theta) * cos(self.alpha), -cos(theta) * sin(self.alpha), self.a * sin(theta)],\n",
        "            [0, sin(self.alpha), cos(self.alpha), self.d],\n",
        "            [0, 0, 0, 1]\n",
        "        ])\n",
        "        return transformation_matrix\n",
        "\n",
        "class Manipulator:\n",
        "    def __init__(self, links=None, q=None):\n",
        "        self.links = links if links else []\n",
        "        self.q = q if q else []\n",
        "    \n",
        "    def add_link(self, link):\n",
        "        self.links.append(link)\n",
        "\n",
        "    def forward_kinematics(self, joint_angles):        \n",
        "        transformation_matrix = np.identity(4)\n",
        "        pos = []\n",
        "        for i in range(len(self.links)):\n",
        "            transformation_matrix = np.dot(transformation_matrix, self.links[i].get_transformation(joint_angles[i]))\n",
        "            pos.append([transformation_matrix[0][3],transformation_matrix[1][3]])\n",
        "        return transformation_matrix"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initiating a 2-Joints Manipulator**\n",
        "\n",
        "The class \"Manipulator\" created was then used to create an instant of a 2 joints model. A sketch of the first manipulator considered can be seen below along with it's DH parameters required to set it up.\n",
        "\n",
        "*Attach image of manipulator*\n",
        "\n",
        "*Attach DH Parameters table*"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_manipulator_plot(self,x_joint1,y_joint1,x_joint2,y_joint2):\n",
        "\n",
        "        # Plotting the manipulator links\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        plt.plot([0, x_joint1], [0, y_joint1], 'bo-', label='Link 1')\n",
        "        plt.plot([x_joint1, x_joint2], [y_joint1, y_joint2], 'ro-', label='Link 2')\n",
        "\n",
        "        # Plotting the joints\n",
        "        plt.plot(0, 0, 'go', label='Base Joint (0, 0)')\n",
        "        plt.plot(x_joint1, y_joint1, 'bo', label='Joint 1')\n",
        "        plt.plot(x_joint2, y_joint2, 'ro', label='End Effector')\n",
        "\n",
        "        plt.xlabel('X-axis')\n",
        "        plt.ylabel('Y-axis')\n",
        "        plt.title('2D Manipulator with Joint Positions')\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "        plt.axis('equal')\n",
        "        plt.show()\n",
        "\n",
        "        return plt;"
      ],
      "outputs": [],
      "metadata": {
        "id": "mVFE1mx_23bo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**0. Necessary Setup**"
      ],
      "metadata": {
        "id": "jddIeJV4gq3-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Using cuda GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "uzDGfun2gqWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Getting the required data**"
      ],
      "metadata": {
        "id": "AijMScVdfU7R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Loading the Data\n",
        "x_data = []\n",
        "y_data = []\n",
        "with open('fk_data.txt', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "    for line in lines:\n",
        "        data = line.split()\n",
        "        x_data.append([float(data[0]),float(data[1])]);\n",
        "        y_data.append([float(data[2]),float(data[3])]);\n",
        "q = np.array(x_data)\n",
        "p = np.array(y_data)\n",
        "print(\"q Shape: \", q.shape)\n",
        "print(\"p Shape: \", p.shape)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q Shape:  (99068, 2)\n",
            "p Shape:  (99068, 2)\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6ECRAfphvPt",
        "outputId": "42148e56-e762-4622-e889-9209e7854281"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X = p;\n",
        "y = q;"
      ],
      "outputs": [],
      "metadata": {
        "id": "wzM20ij72iTZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Shuffling the data\n",
        "combined_data = list(zip(X, y))\n",
        "np.random.shuffle(combined_data)\n",
        "shuffled_X, shuffled_y = zip(*combined_data)\n",
        "X = np.array(shuffled_X)\n",
        "y = np.array(shuffled_y)\n",
        "X.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(99068, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAo0skXBpg3q",
        "outputId": "d7eacd93-7dbf-41fa-aac0-0d91c2e04a79"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class PosesDataset(Dataset):\n",
        "\n",
        "    def __init__(self, x, y , transform = None):\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "        self.q = np.asarray(q)\n",
        "        self.p = np.asarray(p)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        q = self.q[index]\n",
        "        p = self.p[index]\n",
        "\n",
        "        q = torch.tensor(q)\n",
        "        p = torch.tensor(p)\n",
        "\n",
        "        return p, q\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.p)"
      ],
      "outputs": [],
      "metadata": {
        "id": "dazT7iYyfTH6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=32)\n",
        "\n",
        "train_set = PosesDataset(X_train, y_train)\n",
        "test_set = PosesDataset(X_test, y_test)\n",
        "\n",
        "print(\"Train Set Size: \", len(train_set))\n",
        "print(\"Test Set Size: \", len(test_set))\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=1024, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=1024, shuffle=False)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Set Size:  99068\n",
            "Test Set Size:  99068\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tqq4iE9Xut3o",
        "outputId": "0af355ff-bbc6-4541-c467-64d22e897033"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class IKNN(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(IKNN, self).__init__()\n",
        "\n",
        "    input_size = 2  # Input size (array with 2 values)\n",
        "    output_size = 2  # Output size (array of 2 values)\n",
        "    hidden_size = 400  # Hidden layer size\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(input_size, hidden_size),  # Input layer\n",
        "        nn.Sigmoid(),  # Activation function\n",
        "\n",
        "        nn.Linear(hidden_size, 256),  # Additional layer\n",
        "        nn.Sigmoid(),  # Activation function\n",
        "\n",
        "        nn.Linear(256, 64),  # Additional layer\n",
        "        nn.Sigmoid(),  # Activation function\n",
        "\n",
        "        nn.Linear(64, output_size)  # Output layer\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)"
      ],
      "outputs": [],
      "metadata": {
        "id": "IfGEh9NcA2em"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model = IKNN()\n",
        "model.to(device)\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "print(model)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IKNN(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=2, out_features=400, bias=True)\n",
            "    (1): Sigmoid()\n",
            "    (2): Linear(in_features=400, out_features=256, bias=True)\n",
            "    (3): Sigmoid()\n",
            "    (4): Linear(in_features=256, out_features=64, bias=True)\n",
            "    (5): Sigmoid()\n",
            "    (6): Linear(in_features=64, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9RSXYaggAdw",
        "outputId": "3a4c7a25-3731-40a6-d53b-165e96854ffd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Training the model**"
      ],
      "metadata": {
        "id": "cXF7R2YhFkKv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "num_of_epochs = 500\n",
        "train_loss_history = []\n",
        "test_loss_history = []\n",
        "train_accuracy_history = []\n",
        "test_accuracy_history = []\n",
        "\n",
        "for epoch in range(num_of_epochs):\n",
        "  train_loss = 0.0\n",
        "  test_loss = 0.0\n",
        "  train_accuracy = 0.0\n",
        "  test_accuracy = 0.0\n",
        "  for p, q in train_loader:\n",
        "    p, q = p.to(device).float(), q.to(device).float()\n",
        "    # Forward Pass\n",
        "    output = model(p)\n",
        "    # Finding Loss\n",
        "    loss = nn.functional.mse_loss(output,q)\n",
        "    # Optimizing Loss\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "    error = torch.abs(output - q)\n",
        "    error_norm = torch.linalg.norm(error, ord=2).sum()\n",
        "    train_accuracy += error_norm/len(p)\n",
        "  for p, q in test_loader:\n",
        "    with torch.no_grad():\n",
        "      p, q = p.to(device).float(), q.to(device).float()\n",
        "      # Forward Pass\n",
        "      output = model(p)\n",
        "      # Finding Loss\n",
        "      loss = nn.functional.mse_loss(output,q)\n",
        "      test_loss += loss.item()\n",
        "      error = torch.abs(output - q)\n",
        "      error_norm = torch.linalg.norm(error, ord=2).sum()\n",
        "      test_accuracy += error_norm/len(p)\n",
        "  train_loss = train_loss/len(train_loader)\n",
        "  test_loss = test_loss/len(test_loader)\n",
        "  train_accuracy = train_accuracy/len(test_loader)\n",
        "  test_accuracy = test_accuracy/len(train_loader)\n",
        "  train_loss_history.append(train_loss)\n",
        "  test_loss_history.append(test_loss)\n",
        "  train_accuracy_history.append(train_accuracy)\n",
        "  test_accuracy_history.append(test_accuracy)\n",
        "  print(f\"Epoch: {epoch}, Training Loss: {train_loss:.2f}, Training Accuracy: {train_accuracy:.4f}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Training Loss: 4.57, Training Accuracy: 0.0826\n",
            "Epoch: 1, Training Loss: 3.81, Training Accuracy: 0.0801\n",
            "Epoch: 2, Training Loss: 3.80, Training Accuracy: 0.0799\n",
            "Epoch: 3, Training Loss: 3.61, Training Accuracy: 0.0772\n",
            "Epoch: 4, Training Loss: 3.25, Training Accuracy: 0.0718\n",
            "Epoch: 5, Training Loss: 3.13, Training Accuracy: 0.0706\n",
            "Epoch: 6, Training Loss: 3.10, Training Accuracy: 0.0701\n",
            "Epoch: 7, Training Loss: 3.07, Training Accuracy: 0.0698\n",
            "Epoch: 8, Training Loss: 3.05, Training Accuracy: 0.0695\n",
            "Epoch: 9, Training Loss: 3.03, Training Accuracy: 0.0691\n",
            "Epoch: 10, Training Loss: 3.02, Training Accuracy: 0.0688\n",
            "Epoch: 11, Training Loss: 3.00, Training Accuracy: 0.0685\n",
            "Epoch: 12, Training Loss: 2.98, Training Accuracy: 0.0683\n",
            "Epoch: 13, Training Loss: 2.98, Training Accuracy: 0.0682\n",
            "Epoch: 14, Training Loss: 2.97, Training Accuracy: 0.0681\n",
            "Epoch: 15, Training Loss: 2.97, Training Accuracy: 0.0681\n",
            "Epoch: 16, Training Loss: 2.96, Training Accuracy: 0.0680\n",
            "Epoch: 17, Training Loss: 2.96, Training Accuracy: 0.0679\n",
            "Epoch: 18, Training Loss: 2.95, Training Accuracy: 0.0679\n",
            "Epoch: 19, Training Loss: 2.95, Training Accuracy: 0.0679\n",
            "Epoch: 20, Training Loss: 2.95, Training Accuracy: 0.0679\n",
            "Epoch: 21, Training Loss: 2.95, Training Accuracy: 0.0678\n",
            "Epoch: 22, Training Loss: 2.94, Training Accuracy: 0.0678\n",
            "Epoch: 23, Training Loss: 2.94, Training Accuracy: 0.0678\n",
            "Epoch: 24, Training Loss: 2.94, Training Accuracy: 0.0678\n",
            "Epoch: 25, Training Loss: 2.94, Training Accuracy: 0.0678\n",
            "Epoch: 26, Training Loss: 2.94, Training Accuracy: 0.0678\n",
            "Epoch: 27, Training Loss: 2.94, Training Accuracy: 0.0677\n",
            "Epoch: 28, Training Loss: 2.94, Training Accuracy: 0.0678\n",
            "Epoch: 29, Training Loss: 2.94, Training Accuracy: 0.0677\n",
            "Epoch: 30, Training Loss: 2.94, Training Accuracy: 0.0677\n",
            "Epoch: 31, Training Loss: 2.94, Training Accuracy: 0.0678\n",
            "Epoch: 32, Training Loss: 2.93, Training Accuracy: 0.0677\n",
            "Epoch: 33, Training Loss: 2.93, Training Accuracy: 0.0677\n",
            "Epoch: 34, Training Loss: 2.93, Training Accuracy: 0.0677\n",
            "Epoch: 35, Training Loss: 2.94, Training Accuracy: 0.0677\n",
            "Epoch: 36, Training Loss: 2.93, Training Accuracy: 0.0677\n",
            "Epoch: 37, Training Loss: 2.93, Training Accuracy: 0.0676\n",
            "Epoch: 38, Training Loss: 2.93, Training Accuracy: 0.0676\n",
            "Epoch: 39, Training Loss: 2.93, Training Accuracy: 0.0676\n",
            "Epoch: 40, Training Loss: 2.93, Training Accuracy: 0.0676\n",
            "Epoch: 41, Training Loss: 2.93, Training Accuracy: 0.0677\n",
            "Epoch: 42, Training Loss: 2.93, Training Accuracy: 0.0676\n",
            "Epoch: 43, Training Loss: 2.93, Training Accuracy: 0.0676\n",
            "Epoch: 44, Training Loss: 2.93, Training Accuracy: 0.0676\n",
            "Epoch: 45, Training Loss: 2.93, Training Accuracy: 0.0676\n",
            "Epoch: 46, Training Loss: 2.93, Training Accuracy: 0.0676\n",
            "Epoch: 47, Training Loss: 2.93, Training Accuracy: 0.0676\n",
            "Epoch: 48, Training Loss: 2.93, Training Accuracy: 0.0676\n",
            "Epoch: 49, Training Loss: 2.93, Training Accuracy: 0.0676\n",
            "Epoch: 50, Training Loss: 2.92, Training Accuracy: 0.0676\n",
            "Epoch: 51, Training Loss: 2.93, Training Accuracy: 0.0676\n",
            "Epoch: 52, Training Loss: 2.93, Training Accuracy: 0.0676\n",
            "Epoch: 53, Training Loss: 2.93, Training Accuracy: 0.0676\n",
            "Epoch: 54, Training Loss: 2.93, Training Accuracy: 0.0676\n",
            "Epoch: 55, Training Loss: 2.93, Training Accuracy: 0.0676\n",
            "Epoch: 56, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 57, Training Loss: 2.92, Training Accuracy: 0.0676\n",
            "Epoch: 58, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 59, Training Loss: 2.93, Training Accuracy: 0.0676\n",
            "Epoch: 60, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 61, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 62, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 63, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 64, Training Loss: 2.92, Training Accuracy: 0.0676\n",
            "Epoch: 65, Training Loss: 2.93, Training Accuracy: 0.0676\n",
            "Epoch: 66, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 67, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 68, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 69, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 70, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 71, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 72, Training Loss: 2.93, Training Accuracy: 0.0676\n",
            "Epoch: 73, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 74, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 75, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 76, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 77, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 78, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 79, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 80, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 81, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 82, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 83, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 84, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 85, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 86, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 87, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 88, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 89, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 90, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 91, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 92, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 93, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 94, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 95, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 96, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 97, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 98, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 99, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 100, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 101, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 102, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 103, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 104, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 105, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 106, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 107, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 108, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 109, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 110, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 111, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 112, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 113, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 114, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 115, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 116, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 117, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 118, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 119, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 120, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 121, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 122, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 123, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 124, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 125, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 126, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 127, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 128, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 129, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 130, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 131, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 132, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 133, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 134, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 135, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 136, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 137, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 138, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 139, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 140, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 141, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 142, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 143, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 144, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 145, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 146, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 147, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 148, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 149, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 150, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 151, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 152, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 153, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 154, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 155, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 156, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 157, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 158, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 159, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 160, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 161, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 162, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 163, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 164, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 165, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 166, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 167, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 168, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 169, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 170, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 171, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 172, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 173, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 174, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 175, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 176, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 177, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 178, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 179, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 180, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 181, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 182, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 183, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 184, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 185, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 186, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 187, Training Loss: 2.92, Training Accuracy: 0.0675\n",
            "Epoch: 188, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 189, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 190, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 191, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 192, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 193, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 194, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 195, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 196, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 197, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 198, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 199, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 200, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 201, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 202, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 203, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 204, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 205, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 206, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 207, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 208, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 209, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 210, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 211, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 212, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 213, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 214, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 215, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 216, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 217, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 218, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 219, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 220, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 221, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 222, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 223, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 224, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 225, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 226, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 227, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 228, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 229, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 230, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 231, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 232, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 233, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 234, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 235, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 236, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 237, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 238, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 239, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 240, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 241, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 242, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 243, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 244, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 245, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 246, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 247, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 248, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 249, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 250, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 251, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 252, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 253, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 254, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 255, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 256, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 257, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 258, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 259, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 260, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 261, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 262, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 263, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 264, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 265, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 266, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 267, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 268, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 269, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 270, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 271, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 272, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 273, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 274, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 275, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 276, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 277, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 278, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 279, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 280, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 281, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 282, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 283, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 284, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 285, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 286, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 287, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 288, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 289, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 290, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 291, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 292, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 293, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 294, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 295, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 296, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 297, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 298, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 299, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 300, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 301, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 302, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 303, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 304, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 305, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 306, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 307, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 308, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 309, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 310, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 311, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 312, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 313, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 314, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 315, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 316, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 317, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 318, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 319, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 320, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 321, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 322, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 323, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 324, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 325, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 326, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 327, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 328, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 329, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 330, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 331, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 332, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 333, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 334, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 335, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 336, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 337, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 338, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 339, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 340, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 341, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 342, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 343, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 344, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 345, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 346, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 347, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 348, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 349, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 350, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 351, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 352, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 353, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 354, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 355, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 356, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 357, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 358, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 359, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 360, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 361, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 362, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 363, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 364, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 365, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 366, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 367, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 368, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 369, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 370, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 371, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 372, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 373, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 374, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 375, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 376, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 377, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 378, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 379, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 380, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 381, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 382, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 383, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 384, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 385, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 386, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 387, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 388, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 389, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 390, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 391, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 392, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 393, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 394, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 395, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 396, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 397, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 398, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 399, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 400, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 401, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 402, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 403, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 404, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 405, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 406, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 407, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 408, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 409, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 410, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 411, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 412, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 413, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 414, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 415, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 416, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 417, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 418, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 419, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 420, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 421, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 422, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 423, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 424, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 425, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 426, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 427, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 428, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 429, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 430, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 431, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 432, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 433, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 434, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 435, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 436, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 437, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 438, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 439, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 440, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 441, Training Loss: 2.92, Training Accuracy: 0.0674\n",
            "Epoch: 442, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 443, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 444, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 445, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 446, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 447, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 448, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 449, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 450, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 451, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 452, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 453, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 454, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 455, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 456, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 457, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 458, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 459, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 460, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 461, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 462, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 463, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 464, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 465, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 466, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 467, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 468, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 469, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 470, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 471, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 472, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 473, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 474, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 475, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 476, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 477, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 478, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 479, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 480, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 481, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 482, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 483, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 484, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 485, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 486, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 487, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 488, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 489, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 490, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 491, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 492, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 493, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 494, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 495, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 496, Training Loss: 2.90, Training Accuracy: 0.0673\n",
            "Epoch: 497, Training Loss: 2.91, Training Accuracy: 0.0673\n",
            "Epoch: 498, Training Loss: 2.91, Training Accuracy: 0.0674\n",
            "Epoch: 499, Training Loss: 2.90, Training Accuracy: 0.0673\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "slZEBM3gEbCB",
        "outputId": "bbb564a6-6a4c-4fc7-8149-56ff389e2bd0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "fig = plt.figure()\n",
        "plt.plot(range(30),train_accuracy_history,\"-\",linewidth=3,label=\"Train Error\")\n",
        "plt.plot(range(30),test_accuracy_history,\"-\",linewidth=3,label=\"Test Error\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-26de1d6b25e3>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_accuracy_history\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Train Error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_accuracy_history\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Test Error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2810\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2811\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2812\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2813\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2814\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \"\"\"\n\u001b[1;32m   1687\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             yield from self._plot_args(\n\u001b[0m\u001b[1;32m    312\u001b[0m                 this, kwargs, ambiguous_fmt_datakey=ambiguous_fmt_datakey)\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_check_1d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1346\u001b[0m             \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m             len(x.shape) < 1):\n\u001b[0;32m-> 1348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1028\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1030\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi4AAAGiCAYAAADA0E3hAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAcw0lEQVR4nO3db2zdVf3A8U/b0VsItEzn2m0WKyiiAhturBYkiKk2gUz3wDjBbHPhj+AkuEZlY7CK6DoRyKIrLkwQH6ibEDDGLUOsLgapWdjWBGSDwMBNYwsT184iLWu/vweG+qvrYLf0z077eiX3wY7n3O+5Hkbf3H8tyLIsCwCABBSO9QYAAI6VcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSkXe4/OEPf4h58+bF9OnTo6CgIH75y1++5Zpt27bFRz7ykcjlcvG+970v7r///iFsFQCY6PIOl66urpg5c2Y0NTUd0/wXXnghLrvssrjkkkuitbU1vvrVr8ZVV10VjzzySN6bBQAmtoK380sWCwoK4uGHH4758+cfdc6NN94Ymzdvjqeeeqp/7POf/3wcPHgwtm7dOtRLAwAT0KSRvkBLS0vU1tYOGKurq4uvfvWrR13T3d0d3d3d/X/u6+uLV155Jd75zndGQUHBSG0VABhGWZbFoUOHYvr06VFYODxvqx3xcGlra4vy8vIBY+Xl5dHZ2Rn//ve/48QTTzxiTWNjY9x6660jvTUAYBTs378/3v3udw/LfY14uAzFihUror6+vv/PHR0dcdppp8X+/fujtLR0DHcGAByrzs7OqKysjFNOOWXY7nPEw6WioiLa29sHjLW3t0dpaemgz7ZERORyucjlckeMl5aWChcASMxwvs1jxL/HpaamJpqbmweMPfroo1FTUzPSlwYAxpm8w+Vf//pXtLa2Rmtra0T85+POra2tsW/fvoj4z8s8ixYt6p9/7bXXxt69e+Mb3/hG7NmzJ+6+++74xS9+EcuWLRueRwAATBh5h8sTTzwR5513Xpx33nkREVFfXx/nnXderFq1KiIi/v73v/dHTETEe9/73ti8eXM8+uijMXPmzLjzzjvjRz/6UdTV1Q3TQwAAJoq39T0uo6WzszPKysqio6PDe1wAIBEj8fPb7yoCAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZQwqXpqamqKqqipKSkqiuro7t27e/6fy1a9fGBz7wgTjxxBOjsrIyli1bFq+99tqQNgwATFx5h8umTZuivr4+GhoaYufOnTFz5syoq6uLl156adD5P/vZz2L58uXR0NAQu3fvjnvvvTc2bdoUN91009vePAAwseQdLnfddVdcffXVsWTJkvjQhz4U69evj5NOOinuu+++Qec//vjjceGFF8YVV1wRVVVV8alPfSouv/zyt3yWBgDgf+UVLj09PbFjx46ora397x0UFkZtbW20tLQMuuaCCy6IHTt29IfK3r17Y8uWLXHppZce9Trd3d3R2dk54AYAMCmfyQcOHIje3t4oLy8fMF5eXh579uwZdM0VV1wRBw4ciI997GORZVkcPnw4rr322jd9qaixsTFuvfXWfLYGAEwAI/6pom3btsXq1avj7rvvjp07d8ZDDz0Umzdvjttuu+2oa1asWBEdHR39t/3794/0NgGABOT1jMuUKVOiqKgo2tvbB4y3t7dHRUXFoGtuueWWWLhwYVx11VUREXHOOedEV1dXXHPNNbFy5cooLDyynXK5XORyuXy2BgBMAHk941JcXByzZ8+O5ubm/rG+vr5obm6OmpqaQde8+uqrR8RJUVFRRERkWZbvfgGACSyvZ1wiIurr62Px4sUxZ86cmDt3bqxduza6urpiyZIlERGxaNGimDFjRjQ2NkZExLx58+Kuu+6K8847L6qrq+O5556LW265JebNm9cfMAAAxyLvcFmwYEG8/PLLsWrVqmhra4tZs2bF1q1b+9+wu2/fvgHPsNx8881RUFAQN998c/ztb3+Ld73rXTFv3rz4zne+M3yPAgCYEAqyBF6v6ezsjLKysujo6IjS0tKx3g4AcAxG4ue331UEACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhhQuTU1NUVVVFSUlJVFdXR3bt29/0/kHDx6MpUuXxrRp0yKXy8WZZ54ZW7ZsGdKGAYCJa1K+CzZt2hT19fWxfv36qK6ujrVr10ZdXV0888wzMXXq1CPm9/T0xCc/+cmYOnVqPPjggzFjxoz4y1/+Eqeeeupw7B8AmEAKsizL8llQXV0d559/fqxbty4iIvr6+qKysjKuv/76WL58+RHz169fH9/73vdiz549ccIJJwxpk52dnVFWVhYdHR1RWlo6pPsAAEbXSPz8zuulop6entixY0fU1tb+9w4KC6O2tjZaWloGXfOrX/0qampqYunSpVFeXh5nn312rF69Onp7e496ne7u7ujs7BxwAwDIK1wOHDgQvb29UV5ePmC8vLw82traBl2zd+/eePDBB6O3tze2bNkSt9xyS9x5553x7W9/+6jXaWxsjLKysv5bZWVlPtsEAMapEf9UUV9fX0ydOjXuueeemD17dixYsCBWrlwZ69evP+qaFStWREdHR/9t//79I71NACABeb05d8qUKVFUVBTt7e0Dxtvb26OiomLQNdOmTYsTTjghioqK+sc++MEPRltbW/T09ERxcfERa3K5XORyuXy2BgBMAHk941JcXByzZ8+O5ubm/rG+vr5obm6OmpqaQddceOGF8dxzz0VfX1//2LPPPhvTpk0bNFoAAI4m75eK6uvrY8OGDfGTn/wkdu/eHdddd110dXXFkiVLIiJi0aJFsWLFiv751113Xbzyyitxww03xLPPPhubN2+O1atXx9KlS4fvUQAAE0Le3+OyYMGCePnll2PVqlXR1tYWs2bNiq1bt/a/YXffvn1RWPjfHqqsrIxHHnkkli1bFueee27MmDEjbrjhhrjxxhuH71EAABNC3t/jMhZ8jwsApGfMv8cFAGAsCRcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIxpDCpampKaqqqqKkpCSqq6tj+/btx7Ru48aNUVBQEPPnzx/KZQGACS7vcNm0aVPU19dHQ0ND7Ny5M2bOnBl1dXXx0ksvvem6F198Mb72ta/FRRddNOTNAgATW97hctddd8XVV18dS5YsiQ996EOxfv36OOmkk+K+++476pre3t74whe+ELfeemucfvrpb3mN7u7u6OzsHHADAMgrXHp6emLHjh1RW1v73zsoLIza2tpoaWk56rpvfetbMXXq1LjyyiuP6TqNjY1RVlbWf6usrMxnmwDAOJVXuBw4cCB6e3ujvLx8wHh5eXm0tbUNuuaxxx6Le++9NzZs2HDM11mxYkV0dHT03/bv35/PNgGAcWrSSN75oUOHYuHChbFhw4aYMmXKMa/L5XKRy+VGcGcAQIryCpcpU6ZEUVFRtLe3Dxhvb2+PioqKI+Y///zz8eKLL8a8efP6x/r6+v5z4UmT4plnnokzzjhjKPsGACagvF4qKi4ujtmzZ0dzc3P/WF9fXzQ3N0dNTc0R888666x48skno7W1tf/26U9/Oi655JJobW313hUAIC95v1RUX18fixcvjjlz5sTcuXNj7dq10dXVFUuWLImIiEWLFsWMGTOisbExSkpK4uyzzx6w/tRTT42IOGIcAOCt5B0uCxYsiJdffjlWrVoVbW1tMWvWrNi6dWv/G3b37dsXhYW+kBcAGH4FWZZlY72Jt9LZ2RllZWXR0dERpaWlY70dAOAYjMTPb0+NAADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQjCGFS1NTU1RVVUVJSUlUV1fH9u3bjzp3w4YNcdFFF8XkyZNj8uTJUVtb+6bzAQCOJu9w2bRpU9TX10dDQ0Ps3LkzZs6cGXV1dfHSSy8NOn/btm1x+eWXx+9///toaWmJysrK+NSnPhV/+9vf3vbmAYCJpSDLsiyfBdXV1XH++efHunXrIiKir68vKisr4/rrr4/ly5e/5fre3t6YPHlyrFu3LhYtWjTonO7u7uju7u7/c2dnZ1RWVkZHR0eUlpbms10AYIx0dnZGWVnZsP78zusZl56entixY0fU1tb+9w4KC6O2tjZaWlqO6T5effXVeP311+Md73jHUec0NjZGWVlZ/62ysjKfbQIA41Re4XLgwIHo7e2N8vLyAePl5eXR1tZ2TPdx4403xvTp0wfEz/9asWJFdHR09N/279+fzzYBgHFq0mhebM2aNbFx48bYtm1blJSUHHVeLpeLXC43ijsDAFKQV7hMmTIlioqKor29fcB4e3t7VFRUvOnaO+64I9asWRO//e1v49xzz81/pwDAhJfXS0XFxcUxe/bsaG5u7h/r6+uL5ubmqKmpOeq622+/PW677bbYunVrzJkzZ+i7BQAmtLxfKqqvr4/FixfHnDlzYu7cubF27dro6uqKJUuWRETEokWLYsaMGdHY2BgREd/97ndj1apV8bOf/Syqqqr63wtz8sknx8knnzyMDwUAGO/yDpcFCxbEyy+/HKtWrYq2traYNWtWbN26tf8Nu/v27YvCwv8+kfPDH/4wenp64rOf/eyA+2loaIhvfvObb2/3AMCEkvf3uIyFkfgcOAAwssb8e1wAAMaScAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkDClcmpqaoqqqKkpKSqK6ujq2b9/+pvMfeOCBOOuss6KkpCTOOeec2LJly5A2CwBMbHmHy6ZNm6K+vj4aGhpi586dMXPmzKirq4uXXnpp0PmPP/54XH755XHllVfGrl27Yv78+TF//vx46qmn3vbmAYCJpSDLsiyfBdXV1XH++efHunXrIiKir68vKisr4/rrr4/ly5cfMX/BggXR1dUVv/71r/vHPvrRj8asWbNi/fr1g16ju7s7uru7+//c0dERp512Wuzfvz9KS0vz2S4AMEY6OzujsrIyDh48GGVlZcNyn5PymdzT0xM7duyIFStW9I8VFhZGbW1ttLS0DLqmpaUl6uvrB4zV1dXFL3/5y6Nep7GxMW699dYjxisrK/PZLgBwHPjHP/4xNuFy4MCB6O3tjfLy8gHj5eXlsWfPnkHXtLW1DTq/ra3tqNdZsWLFgNg5ePBgvOc974l9+/YN2wNnaN6oZ89+jT1ncfxwFscX53H8eOMVk3e84x3Ddp95hctoyeVykcvljhgvKyvzD+FxorS01FkcJ5zF8cNZHF+cx/GjsHD4PsSc1z1NmTIlioqKor29fcB4e3t7VFRUDLqmoqIir/kAAEeTV7gUFxfH7Nmzo7m5uX+sr68vmpubo6amZtA1NTU1A+ZHRDz66KNHnQ8AcDR5v1RUX18fixcvjjlz5sTcuXNj7dq10dXVFUuWLImIiEWLFsWMGTOisbExIiJuuOGGuPjii+POO++Myy67LDZu3BhPPPFE3HPPPcd8zVwuFw0NDYO+fMTochbHD2dx/HAWxxfncfwYibPI++PQERHr1q2L733ve9HW1hazZs2K73//+1FdXR0RER//+Mejqqoq7r///v75DzzwQNx8883x4osvxvvf//64/fbb49JLLx22BwEATAxDChcAgLHgdxUBAMkQLgBAMoQLAJAM4QIAJOO4CZempqaoqqqKkpKSqK6uju3bt7/p/AceeCDOOuusKCkpiXPOOSe2bNkySjsd//I5iw0bNsRFF10UkydPjsmTJ0dtbe1bnh3HLt+/F2/YuHFjFBQUxPz580d2gxNIvmdx8ODBWLp0aUybNi1yuVyceeaZ/j01TPI9i7Vr18YHPvCBOPHEE6OysjKWLVsWr7322ijtdvz6wx/+EPPmzYvp06dHQUHBm/4Owjds27YtPvKRj0Qul4v3ve99Az6BfMyy48DGjRuz4uLi7L777sv+/Oc/Z1dffXV26qmnZu3t7YPO/+Mf/5gVFRVlt99+e/b0009nN998c3bCCSdkTz755CjvfPzJ9yyuuOKKrKmpKdu1a1e2e/fu7Itf/GJWVlaW/fWvfx3lnY8/+Z7FG1544YVsxowZ2UUXXZR95jOfGZ3NjnP5nkV3d3c2Z86c7NJLL80ee+yx7IUXXsi2bduWtba2jvLOx598z+KnP/1plsvlsp/+9KfZCy+8kD3yyCPZtGnTsmXLlo3yzsefLVu2ZCtXrsweeuihLCKyhx9++E3n7927NzvppJOy+vr67Omnn85+8IMfZEVFRdnWrVvzuu5xES5z587Nli5d2v/n3t7ebPr06VljY+Og8z/3uc9ll1122YCx6urq7Etf+tKI7nMiyPcs/tfhw4ezU045JfvJT34yUlucMIZyFocPH84uuOCC7Ec/+lG2ePFi4TJM8j2LH/7wh9npp5+e9fT0jNYWJ4x8z2Lp0qXZJz7xiQFj9fX12YUXXjii+5xojiVcvvGNb2Qf/vCHB4wtWLAgq6ury+taY/5SUU9PT+zYsSNqa2v7xwoLC6O2tjZaWloGXdPS0jJgfkREXV3dUedzbIZyFv/r1Vdfjddff31YfxPoRDTUs/jWt74VU6dOjSuvvHI0tjkhDOUsfvWrX0VNTU0sXbo0ysvL4+yzz47Vq1dHb2/vaG17XBrKWVxwwQWxY8eO/peT9u7dG1u2bPElqGNguH52j/lvhz5w4ED09vZGeXn5gPHy8vLYs2fPoGva2toGnd/W1jZi+5wIhnIW/+vGG2+M6dOnH/EPJ/kZylk89thjce+990Zra+so7HDiGMpZ7N27N373u9/FF77whdiyZUs899xz8eUvfzlef/31aGhoGI1tj0tDOYsrrrgiDhw4EB/72Mciy7I4fPhwXHvttXHTTTeNxpb5f472s7uzszP+/e9/x4knnnhM9zPmz7gwfqxZsyY2btwYDz/8cJSUlIz1diaUQ4cOxcKFC2PDhg0xZcqUsd7OhNfX1xdTp06Ne+65J2bPnh0LFiyIlStXxvr168d6axPOtm3bYvXq1XH33XfHzp0746GHHorNmzfHbbfdNtZbY4jG/BmXKVOmRFFRUbS3tw8Yb29vj4qKikHXVFRU5DWfYzOUs3jDHXfcEWvWrInf/va3ce65547kNieEfM/i+eefjxdffDHmzZvXP9bX1xcREZMmTYpnnnkmzjjjjJHd9Dg1lL8X06ZNixNOOCGKior6xz74wQ9GW1tb9PT0RHFx8YjuebwaylnccsstsXDhwrjqqqsiIuKcc86Jrq6uuOaaa2LlypVRWOi/30fL0X52l5aWHvOzLRHHwTMuxcXFMXv27Ghubu4f6+vri+bm5qipqRl0TU1NzYD5ERGPPvroUedzbIZyFhERt99+e9x2222xdevWmDNnzmhsddzL9yzOOuusePLJJ6O1tbX/9ulPfzouueSSaG1tjcrKytHc/rgylL8XF154YTz33HP98RgR8eyzz8a0adNEy9swlLN49dVXj4iTN4Iy86v6RtWw/ezO733DI2Pjxo1ZLpfL7r///uzpp5/OrrnmmuzUU0/N2trasizLsoULF2bLly/vn//HP/4xmzRpUnbHHXdku3fvzhoaGnwcepjkexZr1qzJiouLswcffDD7+9//3n87dOjQWD2EcSPfs/hfPlU0fPI9i3379mWnnHJK9pWvfCV75plnsl//+tfZ1KlTs29/+9tj9RDGjXzPoqGhITvllFOyn//859nevXuz3/zmN9kZZ5yRfe5znxurhzBuHDp0KNu1a1e2a9euLCKyu+66K9u1a1f2l7/8JcuyLFu+fHm2cOHC/vlvfBz661//erZ79+6sqakp3Y9DZ1mW/eAHP8hOO+20rLi4OJs7d272pz/9qf9/u/jii7PFixcPmP+LX/wiO/PMM7Pi4uLswx/+cLZ58+ZR3vH4lc9ZvOc978ki4ohbQ0PD6G98HMr378X/J1yGV75n8fjjj2fV1dVZLpfLTj/99Ow73/lOdvjw4VHe9fiUz1m8/vrr2Te/+c3sjDPOyEpKSrLKysrsy1/+cvbPf/5z9Dc+zvz+978f9N//b/z/v3jx4uziiy8+Ys2sWbOy4uLi7PTTT89+/OMf533dgizzXBkAkIYxf48LAMCxEi4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJCM/wM9kKRvAVrZIAAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WDyqXxtxR2h2",
        "outputId": "a4ff871f-9e88-41a5-9267-163858059c42"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model.eval()  # Set the model to evaluation mode\n",
        "all_predicted = []\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        output = model(images)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        all_predicted.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Calculate accuracy using all_predicted and test_set.labels\n",
        "accuracy = (np.array(all_predicted) == test_set.labels).mean()\n",
        "print(f\"Model Accuracy: {accuracy*100:.2f}%\")"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aC2t65d4TEuc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "torch.save(model.state_dict(),'drive/MyDrive/Colab-Data/rsp.pt')"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VXo0c07PVMxX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(model(torch.tensor([0.854494893105,0.98617294483]).to(device)))"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d-ea-S9obHl3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WfO-MIX7O4Tw"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.10.9 64-bit ('base': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "1d051fc57616189bbb3568453e7ae6a09b39d30a7544d80b45e2dc7e0ced0be8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}